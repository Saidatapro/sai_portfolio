<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sai Teja Boyapati - Resume</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Calibri:ital,wght@0,400;0,700;1,400&display=swap');

        body {
            background-color: #525659;
            margin: 0;
            padding: 40px 0;
            font-family: 'Calibri', sans-serif;
            display: flex;
            justify-content: center;
        }

        .page {
            background-color: white;
            width: 210mm;
            min-height: 297mm;
            padding: 20mm;
            box-sizing: border-box;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.5);
        }

        @media print {
            body {
                background: none;
                padding: 0;
            }

            .page {
                width: 100%;
                box-shadow: none;
                padding: 0;
                margin: 0;
            }

            .print-btn-container {
                display: none !important;
            }
        }

        header {
            text-align: center;
            border-bottom: 2px solid #333;
            padding-bottom: 10px;
            margin-bottom: 20px;
        }

        h1 {
            margin: 0;
            font-size: 24pt;
            text-transform: uppercase;
        }

        .subtitle {
            font-size: 14pt;
            font-weight: bold;
            margin: 5px 0;
        }

        .contact-info {
            font-size: 11pt;
        }

        .contact-info a {
            color: #000;
            text-decoration: none;
        }

        .section {
            margin-bottom: 15px;
        }

        .section-title {
            font-size: 12pt;
            font-weight: bold;
            text-transform: uppercase;
            border-bottom: 1px solid #000;
            margin-bottom: 10px;
            padding-bottom: 2px;
        }

        .content {
            font-size: 10.5pt;
            line-height: 1.3;
            text-align: justify;
        }

        .technical-skills strong {
            font-weight: bold;
        }

        .technical-skills ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .technical-skills li {
            margin-bottom: 4px;
        }

        .education-item {
            margin-bottom: 5px;
        }

        .experience-item {
            margin-bottom: 15px;
        }

        .job-header {
            display: flex;
            justify-content: space-between;
            font-weight: bold;
            font-size: 11pt;
            margin-bottom: 3px;
        }

        .job-role {
            font-style: italic;
        }

        ul {
            margin: 5px 0;
            padding-left: 18px;
        }

        li {
            margin-bottom: 3px;
        }

        /* Print Button Style */
        .print-btn-container {
            position: fixed;
            bottom: 30px;
            right: 30px;
            z-index: 1000;
        }

        .print-btn {
            background-color: #333;
            color: white;
            border: none;
            padding: 12px 24px;
            border-radius: 30px;
            cursor: pointer;
            font-family: sans-serif;
            font-weight: bold;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.4);
            transition: transform 0.2s, background 0.2s;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .print-btn:hover {
            transform: scale(1.05);
            background-color: #555;
        }
    </style>
    <!-- Font Awesome for Icon -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>

<body>

    <div class="page">
        <header>
            <h1>Sai Teja Boyapati</h1>
            <div class="subtitle">Data Engineer (AI & ML)</div>
            <div class="contact-info">
                Casa Grande, AZ | (315) 801-1979 | <a
                    href="mailto:boyapatisaiteja565@gmail.com">boyapatisaiteja565@gmail.com</a> | <a
                    href="https://www.linkedin.com/in/saitejaboyapati999/" target="_blank">LinkedIn</a>
            </div>
        </header>

        <div class="section">
            <div class="section-title">Professional Summary</div>
            <div class="content">
                Data Engineer (AI/ML) with <strong>5+ years of expertise</strong> in designing, developing, and
                optimizing <strong>scalable data pipelines</strong> and <strong>machine learning platforms</strong>
                across cloud environments including <strong>AWS, GCP,</strong> and <strong>Azure</strong>. Proven track
                record of delivering <strong>real-time data solutions</strong> and <strong>high-performance
                    batch</strong>, modernizing <strong>analytics infrastructure</strong>, and enabling
                <strong>end-to-end MLOps</strong> in regulated industries such as <strong>finance, insurance,</strong>
                and <strong>industrial IoT</strong>. Skilled in <strong>big data processing</strong> with Apache Spark,
                PySpark, Hadoop, and <strong>Databricks</strong>, and <strong>real-time streaming</strong> technologies
                like <strong>Kafka, Spark Streaming, Pub/Sub,</strong> and <strong>Azure Stream Analytics</strong>.
                Extensive experience with <strong>data warehousing</strong> and <strong>transformation tools</strong>
                including <strong>Snowflake, BigQuery, Redshift, dbt Core,</strong> and <strong>SQL-based feature
                    engineering</strong>. Strong background in <strong>Python, SQL,</strong> and <strong>ML
                    tooling</strong> such as <strong>SageMaker, Azure ML, RAG, TensorFlow,</strong> and <strong>CI/CD
                    automation</strong> with <strong>Terraform, Docker,</strong> and <strong>GitLab CI</strong>. Adept
                at partnering with <strong>cross-functional teams</strong> to deploy production-grade data products that
                enhance <strong>model performance, reduce infrastructure costs,</strong> and <strong>accelerate ML
                    deployment cycles</strong>. Known for a <strong>pragmatic approach</strong> to data quality,
                pipeline reliability, and production scalability in <strong>Agile/Scrum</strong> environments.
            </div>
        </div>

        <div class="section">
            <div class="section-title">Technical Skills</div>
            <div class="content technical-skills">
                <ul>
                    <li><strong>Programming Languages:</strong> Python, SQL, Java, Scala, Bash</li>
                    <li><strong>Big Data & Distributed Computing:</strong> Apache Spark (PySpark, Spark SQL), Hadoop
                        (HDFS, YARN, MapReduce, Hive), Databricks</li>
                    <li><strong>Streaming & Messaging Systems:</strong> Apache Kafka, Spark Structured Streaming, AWS
                        Kinesis, Google Pub/Sub</li>
                    <li><strong>ETL / ELT & Orchestration Tools:</strong> Apache Airflow, AWS Glue, Azure Data Factory,
                        dbt (Data Build Tool), SSIS, Talend</li>
                    <li><strong>Machine Learning & Feature Engineering:</strong> TensorFlow, PyTorch, scikit-learn,
                        XGBoost, MLlib, pandas, NumPy, feature engineering, hyperparameter optimization, performance
                        monitoring, SLM integration (Llama 3.1), RAG</li>
                    <li><strong>Model Deployment & MLOps:</strong> AWS SageMaker, Azure ML, Vertex AI, TensorFlow
                        Serving, TorchServe, MLflow, Kubeflow</li>
                    <li><strong>DevOps & Automation:</strong> Docker, Kubernetes, Jenkins, Terraform, GitHub Actions,
                        CI/CD pipelines, Infrastructure as Code (IaC)</li>
                    <li><strong>Cloud Platforms:</strong> AWS (S3, EMR, Redshift, Glue, Lambda, Kinesis), Azure (Data
                        Lake, Synapse Analytics, Databricks, Azure ML, Azure Stream Analytics, Data Factory), GCP
                        (BigQuery, Cloud Storage, Dataflow, Pub/Sub, Vertex AI)</li>
                    <li><strong>Databases & Data Warehousing:</strong> PostgreSQL, MySQL, SQL Server, Oracle, MongoDB,
                        Cassandra, DynamoDB, Snowflake</li>
                    <li><strong>Data Visualization & Business Intelligence:</strong> Power BI, Tableau, Looker Studio,
                        Matplotlib</li>
                    <li><strong>Software Development & Methodologies:</strong> Agile (Scrum, SAFe), DataOps,
                        microservices architecture, RESTful APIs, OAuth 2.0, data lakehouse architecture, data modeling,
                        system design</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <div class="section-title">Education</div>
            <div class="content">
                <div class="education-item"><strong>Master’s in Computer Science</strong> | University of Central
                    Missouri, Leesburg, MO, USA.</div>
                <div class="education-item"><strong>Bachelor’s in Computer Science</strong> | Keshav Memorial Institute
                    of Technology, Hyderabad, TG, India.</div>
            </div>
        </div>

        <div class="section">
            <div class="section-title">Professional Experience</div>
            <div class="content">
                <div class="experience-item">
                    <div class="job-header">
                        <span>American Express | <em>Data Engineer (AI & ML)</em></span>
                        <span>Phoenix, AZ | May 2025 – Present</span>
                    </div>
                    <ul>
                        <li>Leveraged <strong>LangChain agents</strong> and advanced <strong>BigQuery SQL</strong> to
                            automate and optimize <strong>feature engineering</strong> for key risk models, accelerating
                            the feature development cycle by <strong>50%</strong> and contributing to a
                            <strong>12%</strong> boost in overall model accuracy.
                        </li>
                        <li>Optimized financial <strong>ETL/ELT performance</strong> by applying <strong>incremental
                                loads</strong> and <strong>parallel processing</strong> using <strong>PySpark</strong>,
                            reducing pipeline runtime by <strong>40%</strong> and <strong>compute costs</strong> by
                            <strong>25%</strong>, while maintaining <strong>strict SLAs</strong> for risk and marketing
                            models.
                        </li>
                        <li>Architected a scalable data platform on <strong>Google Cloud Platform (GCP)</strong> using
                            <strong>BigQuery, Cloud Storage,</strong> and <strong>Dataproc</strong>, centralizing
                            large-scale transaction and customer data to accelerate <strong>AI/ML feature
                                development</strong> across business domains.
                        </li>
                        <li>Automated <strong>MLOps pipelines</strong> with <strong>Cloud Build, Terraform,</strong> and
                            <strong>CI/CD tools</strong>, increasing deployment efficiency by <strong>35%</strong> and
                            ensuring reliability and compliance across <strong>batch</strong> and <strong>real-time ML
                                inference systems</strong>.
                        </li>
                        <li>Developed real-time <strong>ML monitoring dashboards</strong> using <strong>Apache Beam
                                (Dataflow), Looker Studio,</strong> and <strong>Python</strong> to detect model/data
                            drift, enabling <strong>25% faster decision-making</strong> and continuous model performance
                            visibility.</li>
                    </ul>
                </div>

                <div class="experience-item">
                    <div class="job-header">
                        <span>Capital One | <em>Data Engineer</em></span>
                        <span>McLean, VA | March 2024 – April 2025</span>
                    </div>
                    <ul>
                        <li>Integrated a <strong>RAG (Retrieval-Augmented Generation)</strong> system using
                            <strong>Llama 3.1</strong> to query a knowledge base of <strong>10k+ compliance</strong>
                            documents, improving the accuracy of <strong>regulatory Q&A</strong> by <strong>40%</strong>
                            and reducing <strong>research time for analysts</strong> by <strong>60%</strong>.
                        </li>
                        <li>Built and operationalized <strong>real-time ML pipelines</strong> for <strong>fraud
                                detection</strong> using <strong>Kafka, Spark Streaming,</strong> and
                            <strong>SageMaker</strong>, reducing fraudulent transaction processing time <strong>from
                                hours to seconds</strong> and cutting false positives by <strong>25%</strong>.
                        </li>
                        <li>Led modernization of the <strong>enterprise analytics layer</strong> by implementing
                            <strong>dbt Core</strong> with <strong>Snowflake</strong>, enabling <strong>medallion
                                architecture, row-level security,</strong> and <strong>automated data lineage</strong>,
                            which accelerated <strong>self-service analytics</strong> by <strong>50%</strong>.
                        </li>
                        <li>Collaborated with <strong>Machine Learning Engineers, Data Scientists,</strong> and
                            <strong>Compliance teams</strong> to architect a <strong>centralized feature store</strong>,
                            reducing <strong>duplicate data processing</strong> by <strong>30%</strong> and accelerating
                            model development by <strong>25%</strong>.
                        </li>
                        <li>Designed and deployed <strong>20+ scalable serverless</strong> data pipelines on
                            <strong>AWS</strong> using <strong>Step Functions, AWS Glue,</strong> and
                            <strong>Lambda</strong>, improving ETL performance by <strong>40%</strong> and reducing
                            <strong>cloud costs</strong> via intelligent orchestration and auto-scaling.
                        </li>
                        <li>Developed automated <strong>CI/CD pipelines</strong> for <strong>data</strong> and
                            <strong>ML workflows</strong> using <strong>GitLab CI, Terraform,</strong> and
                            <strong>Docker</strong>, reducing production deployment failures by <strong>50%</strong>
                            through seamless <strong>testing, versioning,</strong> and <strong>infrastructure
                                automation</strong>.
                        </li>
                        <li>Processed <strong>large-scale customer</strong> and <strong>transaction data</strong> using
                            <strong>Apache Spark on EMR</strong>, orchestrated with <strong>Apache Airflow</strong>,
                            achieving a <strong>35% reduction in data latency</strong> for critical
                            <strong>compliance</strong> and <strong>executive reporting dashboards</strong>.
                        </li>
                    </ul>
                </div>

                <div class="experience-item">
                    <div class="job-header">
                        <span>Mercury Insurance | <em>Data Engineer</em></span>
                        <span>Los Angeles, CA | August 2022 – February 2024</span>
                    </div>
                    <ul>
                        <li>Implemented a <strong>secure Multi-Party Computation (MPC)</strong> framework using
                            <strong>Python</strong> and the <strong>Sharemind platform</strong> to enable
                            <strong>privacy-preserving analytics</strong> on <strong>combined datasets from reinsurance
                                partners</strong>, unlocking new risk insights without exposing raw proprietary data.
                        </li>
                        <li>Architected and deployed a <strong>centralized ML feature store</strong> using
                            <strong>Databricks Feature Store</strong> and <strong>Delta Lake</strong>, standardizing
                            feature reuse across <strong>10+ machine learning models</strong>, reducing <strong>feature
                                engineering redundancy</strong> by <strong>30%</strong>.
                        </li>
                        <li>Developed <strong>event-driven services</strong> backed by <strong>DynamoDB</strong>,
                            enabling real-time premium calculation and <strong>policy lookup</strong> features in
                            agent-facing portals, reducing <strong>data retrieval latency</strong> by
                            <strong>25%</strong>.
                        </li>
                        <li>Built <strong>automated data validation frameworks</strong> in <strong>Python</strong> using
                            <strong>Pandas, PySpark, Scikit-learn,</strong> and <strong>Great Expectations</strong>,
                            enhancing <strong>actuarial data reliability</strong> by <strong>25%</strong> and saving
                            <strong>15+ hours/week</strong> on manual <strong>QA</strong>.
                        </li>
                        <li>Designed and built <strong>scalable ETL pipelines</strong> using <strong>AWS Glue</strong>
                            and <strong>Amazon Redshift</strong> to automate ingestion from <strong>policy,
                                claims,</strong> and <strong>billing systems</strong>, improving <strong>data
                                availability for actuarial</strong> and <strong>risk analytics</strong> by
                            <strong>35%</strong>.
                        </li>
                        <li>Led <strong>Agile Scrum</strong> ceremonies (sprint planning, daily stand-ups,
                            retrospectives) for the data engineering team, improving <strong>sprint
                                predictability</strong> and reducing <strong>carry-over tasks</strong> by
                            <strong>40%</strong>.
                        </li>
                    </ul>
                </div>

                <div class="experience-item">
                    <div class="job-header">
                        <span>Honeywell | <em>Data Engineer</em></span>
                        <span>India | December 2018 – December 2020</span>
                    </div>
                    <ul>
                        <li>Engineered a <strong>real-time analytics pipeline</strong> using <strong>Azure Stream
                                Analytics</strong> and <strong>Power BI</strong>, enabling live monitoring of
                            <strong>industrial IoT sensor data</strong> and predictive maintenance alerts, reducing
                            <strong>unplanned downtime</strong> by <strong>20%</strong>.
                        </li>
                        <li>Optimized complex data workflows by replacing <strong>Hive queries</strong> with
                            <strong>Apache Spark</strong> and <strong>PySpark</strong>, significantly reducing
                            <strong>processing time</strong> and improving performance for <strong>large-scale data
                                transformation tasks</strong>.
                        </li>
                        <li>Built <strong>RESTful microservices</strong> architecture to enable <strong>distributed
                                service communication</strong>, implementing <strong>API versioning</strong> and
                            <strong>OAuth 2.0</strong> for secure data access and integration with enterprise systems.
                        </li>
                        <li>Designed and implemented <strong>NoSQL data models</strong> using <strong>MongoDB</strong>
                            and <strong>Cassandra</strong>, supporting <strong>high-availability applications</strong>
                            and large volumes of <strong>unstructured sensor data</strong>.</li>
                        <li>Managed data ingestion workflows using <strong>Sqoop</strong> to transfer relational data
                            into the <strong>Hadoop ecosystem</strong>, later migrating to <strong>Azure Data
                                Services</strong> to enhance <strong>scalability, performance,</strong> and
                            <strong>pipeline reliability</strong>.
                        </li>
                        <li>Developed custom <strong>Python</strong> and <strong>Java scripts</strong> for <strong>data
                                cleansing, transformation,</strong> and <strong>validation</strong>, incorporating
                            <strong>user-defined functions (UDFs)</strong> to improve <strong>data integrity</strong>
                            and ensure consistency across ingestion pipelines.
                        </li>
                    </ul>
                </div>
            </div>
        </div>

    </div>

    <!-- Print Button -->
    <div class="print-btn-container">
        <button onclick="window.print()" class="print-btn">
            <i class="fas fa-file-download"></i> Download / Print PDF
        </button>
    </div>

</body>

</html>